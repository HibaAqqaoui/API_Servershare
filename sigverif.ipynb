{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 262177,
          "sourceType": "datasetVersion",
          "datasetId": 107946
        },
        {
          "sourceId": 3573020,
          "sourceType": "datasetVersion",
          "datasetId": 2146686
        }
      ],
      "dockerImageVersionId": 30260,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "sigverif",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HibaAqqaoui/API_drive/blob/main/sigverif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "robinreni_signature_verification_dataset_path = kagglehub.dataset_download('robinreni/signature-verification-dataset')\n",
        "medali1992_pretrained_signature_weights_path = kagglehub.dataset_download('medali1992/pretrained-signature-weights')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_xa1hAzwJq_G",
        "outputId": "b489b396-029c-45fd-989e-760b61308b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/robinreni/signature-verification-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 601M/601M [00:09<00:00, 64.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/medali1992/pretrained-signature-weights?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55.6M/55.6M [00:01<00:00, 39.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "metadata": {
        "id": "NfYNWFsQOfuW",
        "outputId": "bea060e0-c1df-4e72-d048-71a5fe341f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name '*signature-verification-dataset*' 2>/dev/null\n",
        "!find / -name '*pretrained-signature-weights*' 2>/dev/null"
      ],
      "metadata": {
        "id": "uHDqrFZPOpHv",
        "outputId": "94b3e183-1e36-4d65-f089-c519f5911591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset\n",
            "/root/.cache/kagglehub/datasets/medali1992/pretrained-signature-weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "Siamese and contrastive loss can be used for signature verification by training a Siamese neural network to differentiate between genuine and forged signatures. The network consists of two identical branches, one for each of the two signatures being compared. The output of the two branches is then fed into a contrastive loss function, which calculates the difference between the two signatures and penalizes the network if the difference is too small (indicating that the signatures are likely to be genuine) or too large (indicating that the signatures are likely to be forged).\n",
        "\n",
        "To improve the performance of the network, it is important to preprocess the signatures before training. This can include removing the background, centering the signature in a canvas, and resizing the signature to a fixed size (such as 170x242). These preprocessing steps can help to standardize the input to the network and make it easier for the network to learn the features that are important for signature verification.\n",
        "\n",
        "# Acknowledgment\n",
        "- This [link](https://github.com/luizgh/sigver) has the pretrained weights I used for my model. I also took the preprocessing functions."
      ],
      "metadata": {
        "id": "l40CFYInJq_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "_DLrGJ_zJq_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Train data\n",
        "#/root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/train_data.csv\n",
        "train = pd.read_csv('../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/train_data.csv')\n",
        "train.rename(columns={\"1\":\"label\"}, inplace=True)\n",
        "train[\"image_real_paths\"] = train[\"068/09_068.png\"].apply(lambda x: f\"../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/train/{x}\")\n",
        "train[\"image_forged_paths\"] = train[\"068_forg/03_0113068.PNG\"].apply(lambda x: f\"../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/train/{x}\")\n",
        "\n",
        "# Test data\n",
        "test = pd.read_csv('../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/test_data.csv')\n",
        "test.rename(columns={\"1\":\"label\"}, inplace=True)\n",
        "test[\"image_real_paths\"] = test[\"068/09_068.png\"].apply(lambda x: f\"../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/test/{x}\")\n",
        "test[\"image_forged_paths\"] = test[\"068_forg/03_0113068.PNG\"].apply(lambda x: f\"../root/.cache/kagglehub/datasets/robinreni/signature-verification-dataset/versions/2/sign_data/test/{x}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ajLgq121Jq_K"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "swQhC-r4MsFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"label\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:22.057735Z",
          "iopub.execute_input": "2023-05-21T16:30:22.058465Z",
          "iopub.status.idle": "2023-05-21T16:30:22.069122Z",
          "shell.execute_reply.started": "2023-05-21T16:30:22.058425Z",
          "shell.execute_reply": "2023-05-21T16:30:22.067974Z"
        },
        "trusted": true,
        "id": "JdtGwYXRJq_K",
        "outputId": "4fc6ca5b-7678-4b9e-b3ec-466b02a95833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    0.543073\n",
              "1    0.456927\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.543073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.456927</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "test[\"label\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:22.072516Z",
          "iopub.execute_input": "2023-05-21T16:30:22.072962Z",
          "iopub.status.idle": "2023-05-21T16:30:22.086329Z",
          "shell.execute_reply.started": "2023-05-21T16:30:22.072917Z",
          "shell.execute_reply": "2023-05-21T16:30:22.08477Z"
        },
        "trusted": true,
        "id": "62M_N2rgJq_K",
        "outputId": "d2208d2a-c2e9-4c5a-9746-efd6ccac1c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    0.517661\n",
              "0    0.482339\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.517661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.482339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory"
      ],
      "metadata": {
        "id": "OfGggaENJq_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:22.088172Z",
          "iopub.execute_input": "2023-05-21T16:30:22.088837Z",
          "iopub.status.idle": "2023-05-21T16:30:22.097389Z",
          "shell.execute_reply.started": "2023-05-21T16:30:22.088768Z",
          "shell.execute_reply": "2023-05-21T16:30:22.096005Z"
        },
        "trusted": true,
        "id": "DS2D4z2jJq_L"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "u47LebHiJq_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    apex=False\n",
        "    debug=False\n",
        "    print_freq=100\n",
        "    size=128\n",
        "    num_workers=2\n",
        "    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n",
        "    epochs=20\n",
        "    # CosineAnnealingLR params\n",
        "    cosanneal_params={\n",
        "        'T_max':4,\n",
        "        'eta_min':1e-5,\n",
        "        'last_epoch':-1\n",
        "    }\n",
        "    #ReduceLROnPlateau params\n",
        "    reduce_params={\n",
        "        'mode':'min',\n",
        "        'factor':0.2,\n",
        "        'patience':4,\n",
        "        'eps':1e-6,\n",
        "        'verbose':True\n",
        "    }\n",
        "    # CosineAnnealingWarmRestarts params\n",
        "    cosanneal_res_params={\n",
        "        'T_0':3,\n",
        "        'eta_min':1e-6,\n",
        "        'T_mult':1,\n",
        "        'last_epoch':-1\n",
        "    }\n",
        "    onecycle_params={\n",
        "        'pct_start':0.1,\n",
        "        'div_factor':1e2,\n",
        "        'max_lr':1e-3,\n",
        "        'steps_per_epoch':7,\n",
        "        'epochs':7\n",
        "    }\n",
        "    batch_size=32\n",
        "    lr=1e-3\n",
        "    weight_decay=1e-3\n",
        "    canvas_size = (952, 1360)\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "    target_size=train[\"label\"].shape[0]\n",
        "    nfolds=5\n",
        "    trn_folds=[0]\n",
        "    model_name='convnet'     #'vit_base_patch32_224_in21k' 'tf_efficientnetv2_b0' 'resnext50_32x4d' 'tresnet_m'\n",
        "    train=True\n",
        "    early_stop=True\n",
        "    target_col=\"label\"\n",
        "    projection2d=True\n",
        "    fc_dim=512\n",
        "    early_stopping_steps=5\n",
        "    grad_cam=False\n",
        "    seed=42\n",
        "\n",
        "if CFG.debug:\n",
        "    CFG.epochs=1\n",
        "    train=train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:22.099746Z",
          "iopub.execute_input": "2023-05-21T16:30:22.100319Z",
          "iopub.status.idle": "2023-05-21T16:30:22.124193Z",
          "shell.execute_reply.started": "2023-05-21T16:30:22.100276Z",
          "shell.execute_reply": "2023-05-21T16:30:22.122973Z"
        },
        "trusted": true,
        "id": "ahjyW7iPJq_M"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "jGtBxFPWJq_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage import filters, transform\n",
        "from skimage.io import imread\n",
        "from skimage import img_as_ubyte\n",
        "from typing import Tuple\n",
        "\n",
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "\n",
        "\n",
        "import scipy as sp\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "# sometimes, you will have images without an ending bit\n",
        "# this takes care of those kind of (corrupt) images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torchvision.models as models\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import ImageOnlyTransform\n",
        "\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Functions for plotting:\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['image.cmap'] = 'Greys'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:22.127999Z",
          "iopub.execute_input": "2023-05-21T16:30:22.128379Z",
          "iopub.status.idle": "2023-05-21T16:30:22.152278Z",
          "shell.execute_reply.started": "2023-05-21T16:30:22.12834Z",
          "shell.execute_reply": "2023-05-21T16:30:22.151229Z"
        },
        "trusted": true,
        "id": "JVMdDJZ4Jq_M",
        "outputId": "15551ffc-c8ef-4e54-fad9-31c44ca2445b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W&B"
      ],
      "metadata": {
        "id": "_81tIxdsJq_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "aCDJ3NaLJq_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Utils\n",
        "# ====================================================\n",
        "def get_score(y_true, y_pred):\n",
        "    score = accuracy_score(y_true, y_pred)\n",
        "    return score\n",
        "\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed=CFG.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.046618Z",
          "iopub.execute_input": "2023-05-21T16:30:54.047196Z",
          "iopub.status.idle": "2023-05-21T16:30:54.060363Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.047158Z",
          "shell.execute_reply": "2023-05-21T16:30:54.05942Z"
        },
        "trusted": true,
        "id": "C8RnH7uUJq_M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "BTDkHaAoJq_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_signature(img: np.ndarray,\n",
        "                         canvas_size: Tuple[int, int],\n",
        "                         img_size: Tuple[int, int] =(170, 242),\n",
        "                         input_size: Tuple[int, int] =(150, 220)) -> np.ndarray:\n",
        "    \"\"\" Pre-process a signature image, centering it in a canvas, resizing the image and cropping it.\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : np.ndarray (H x W)\n",
        "        The signature image\n",
        "    canvas_size : tuple (H x W)\n",
        "        The size of a canvas where the signature will be centered on.\n",
        "        Should be larger than the signature.\n",
        "    img_size : tuple (H x W)\n",
        "        The size that will be used to resize (rescale) the signature\n",
        "    input_size : tuple (H x W)\n",
        "        The final size of the signature, obtained by croping the center of image.\n",
        "        This is necessary in cases where data-augmentation is used, and the input\n",
        "        to the neural network needs to have a slightly smaller size.\n",
        "    Returns\n",
        "    -------\n",
        "    np.narray (input_size):\n",
        "        The pre-processed image\n",
        "    -------\n",
        "    \"\"\"\n",
        "    img = img.astype(np.uint8)\n",
        "    centered = normalize_image(img, canvas_size)\n",
        "    inverted = 255 - centered\n",
        "    resized = resize_image(inverted, img_size)\n",
        "\n",
        "    if input_size is not None and input_size != img_size:\n",
        "        cropped = crop_center(resized, input_size)\n",
        "    else:\n",
        "        cropped = resized\n",
        "\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def normalize_image(img: np.ndarray,\n",
        "                    canvas_size: Tuple[int, int] = (840, 1360)) -> np.ndarray:\n",
        "    \"\"\" Centers an image in a pre-defined canvas size, and remove\n",
        "    noise using OTSU's method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : np.ndarray (H x W)\n",
        "        The image to be processed\n",
        "    canvas_size : tuple (H x W)\n",
        "        The desired canvas size\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray (H x W)\n",
        "        The normalized image\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Crop the image before getting the center of mass\n",
        "\n",
        "    # Apply a gaussian filter on the image to remove small components\n",
        "    # Note: this is only used to define the limits to crop the image\n",
        "    blur_radius = 2\n",
        "    blurred_image = filters.gaussian(img, blur_radius, preserve_range=True)\n",
        "\n",
        "    # Binarize the image using OTSU's algorithm. This is used to find the center\n",
        "    # of mass of the image, and find the threshold to remove background noise\n",
        "    threshold = filters.threshold_otsu(img)\n",
        "\n",
        "    # Find the center of mass\n",
        "    binarized_image = blurred_image > threshold\n",
        "    r, c = np.where(binarized_image == 0)\n",
        "    r_center = int(r.mean() - r.min())\n",
        "    c_center = int(c.mean() - c.min())\n",
        "\n",
        "    # Crop the image with a tight box\n",
        "    cropped = img[r.min(): r.max(), c.min(): c.max()]\n",
        "\n",
        "    # 2) Center the image\n",
        "    img_rows, img_cols = cropped.shape\n",
        "    max_rows, max_cols = canvas_size\n",
        "\n",
        "    r_start = max_rows // 2 - r_center\n",
        "    c_start = max_cols // 2 - c_center\n",
        "\n",
        "    # Make sure the new image does not go off bounds\n",
        "    # Emit a warning if the image needs to be cropped, since we don't want this\n",
        "    # for most cases (may be ok for feature learning, so we don't raise an error)\n",
        "    if img_rows > max_rows:\n",
        "        # Case 1: image larger than required (height):  Crop.\n",
        "        print('Warning: cropping image. The signature should be smaller than the canvas size')\n",
        "        r_start = 0\n",
        "        difference = img_rows - max_rows\n",
        "        crop_start = difference // 2\n",
        "        cropped = cropped[crop_start:crop_start + max_rows, :]\n",
        "        img_rows = max_rows\n",
        "    else:\n",
        "        extra_r = (r_start + img_rows) - max_rows\n",
        "        # Case 2: centering exactly would require a larger image. relax the centering of the image\n",
        "        if extra_r > 0:\n",
        "            r_start -= extra_r\n",
        "        if r_start < 0:\n",
        "            r_start = 0\n",
        "\n",
        "    if img_cols > max_cols:\n",
        "        # Case 3: image larger than required (width). Crop.\n",
        "        print('Warning: cropping image. The signature should be smaller than the canvas size')\n",
        "        c_start = 0\n",
        "        difference = img_cols - max_cols\n",
        "        crop_start = difference // 2\n",
        "        cropped = cropped[:, crop_start:crop_start + max_cols]\n",
        "        img_cols = max_cols\n",
        "    else:\n",
        "        # Case 4: centering exactly would require a larger image. relax the centering of the image\n",
        "        extra_c = (c_start + img_cols) - max_cols\n",
        "        if extra_c > 0:\n",
        "            c_start -= extra_c\n",
        "        if c_start < 0:\n",
        "            c_start = 0\n",
        "\n",
        "    normalized_image = np.ones((max_rows, max_cols), dtype=np.uint8) * 255\n",
        "    # Add the image to the blank canvas\n",
        "    normalized_image[r_start:r_start + img_rows, c_start:c_start + img_cols] = cropped\n",
        "\n",
        "    # Remove noise - anything higher than the threshold. Note that the image is still grayscale\n",
        "    normalized_image[normalized_image > threshold] = 255\n",
        "\n",
        "    return normalized_image\n",
        "\n",
        "\n",
        "def remove_background(img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Remove noise using OTSU's method.\n",
        "        Parameters\n",
        "        ----------\n",
        "        img : np.ndarray\n",
        "            The image to be processed\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            The image with background removed\n",
        "        \"\"\"\n",
        "\n",
        "        img = img.astype(np.uint8)\n",
        "        # Binarize the image using OTSU's algorithm. This is used to find the center\n",
        "        # of mass of the image, and find the threshold to remove background noise\n",
        "        threshold = filters.threshold_otsu(img)\n",
        "\n",
        "        # Remove noise - anything higher than the threshold. Note that the image is still grayscale\n",
        "        img[img > threshold] = 255\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "def resize_image(img: np.ndarray,\n",
        "                 size: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\" Crops an image to the desired size without stretching it.\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : np.ndarray (H x W)\n",
        "        The image to be cropped\n",
        "    size : tuple (H x W)\n",
        "        The desired size\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The cropped image\n",
        "    \"\"\"\n",
        "    height, width = size\n",
        "\n",
        "    # Check which dimension needs to be cropped\n",
        "    # (assuming the new height-width ratio may not match the original size)\n",
        "    width_ratio = float(img.shape[1]) / width\n",
        "    height_ratio = float(img.shape[0]) / height\n",
        "    if width_ratio > height_ratio:\n",
        "        resize_height = height\n",
        "        resize_width = int(round(img.shape[1] / height_ratio))\n",
        "    else:\n",
        "        resize_width = width\n",
        "        resize_height = int(round(img.shape[0] / width_ratio))\n",
        "\n",
        "    # Resize the image (will still be larger than new_size in one dimension)\n",
        "    img = transform.resize(img, (resize_height, resize_width),\n",
        "                           mode='constant', anti_aliasing=True, preserve_range=True)\n",
        "\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    # Crop to exactly the desired new_size, using the middle of the image:\n",
        "    if width_ratio > height_ratio:\n",
        "        start = int(round((resize_width-width)/2.0))\n",
        "        return img[:, start:start + width]\n",
        "    else:\n",
        "        start = int(round((resize_height-height)/2.0))\n",
        "        return img[start:start + height, :]\n",
        "\n",
        "\n",
        "def crop_center(img: np.ndarray,\n",
        "                size: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\" Crops the center of an image\n",
        "        Parameters\n",
        "        ----------\n",
        "        img : np.ndarray (H x W)\n",
        "            The image to be cropped\n",
        "        size: tuple (H x W)\n",
        "            The desired size\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            The cRecentropped image\n",
        "        \"\"\"\n",
        "    img_shape = img.shape\n",
        "    start_y = (img_shape[0] - size[0]) // 2\n",
        "    start_x = (img_shape[1] - size[1]) // 2\n",
        "    cropped = img[start_y: start_y + size[0], start_x:start_x + size[1]]\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def crop_center_multiple(imgs: np.ndarray,\n",
        "                         size: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\" Crops the center of multiple images\n",
        "        Parameters\n",
        "        ----------\n",
        "        imgs : np.ndarray (N x C x H_old x W_old)\n",
        "            The images to be cropped\n",
        "        size: tuple (H x W)\n",
        "            The desired size\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray (N x C x H x W)\n",
        "            The cropped images\n",
        "        \"\"\"\n",
        "    img_shape = imgs.shape[2:]\n",
        "    start_y = (img_shape[0] - size[0]) // 2\n",
        "    start_x = (img_shape[1] - size[1]) // 2\n",
        "    cropped = imgs[:, :, start_y: start_y + size[0], start_x:start_x + size[1]]\n",
        "    return cropped\n",
        "\n",
        "def load_signature(path):\n",
        "    return img_as_ubyte(imread(path, as_gray=True))\n",
        "\n",
        "def imshow(img, text=None, save=False):\n",
        "    npimg = img.numpy()\n",
        "    plt.axis('off')\n",
        "\n",
        "    if text:\n",
        "        plt.text(75, 8, text, style='italic', fontweight='bold',\n",
        "             bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "    #plt.imshow(np.transpose(npimg, (1,2,0)), cmap='gray')\n",
        "    plt.imshow(npimg[1, :, :])\n",
        "    plt.show()\n",
        "\n",
        "def show_plot(iteration, loss):\n",
        "    plt.plt(iteration, loss)\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.068804Z",
          "iopub.execute_input": "2023-05-21T16:30:54.07123Z",
          "iopub.status.idle": "2023-05-21T16:30:54.127235Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.071188Z",
          "shell.execute_reply": "2023-05-21T16:30:54.125956Z"
        },
        "trusted": true,
        "id": "FiiGGuWgJq_N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "canvas_size = (952, 1360)\n",
        "img_size = (128, 128)\n",
        "input_size = (256, 256)\n",
        "\n",
        "observation = train.iloc[10]\n",
        "img1 = load_signature(observation[\"image_real_paths\"])\n",
        "img2 = load_signature(observation[\"image_forged_paths\"])\n",
        "preprocessed_img1 = preprocess_signature(img1, canvas_size, input_size)\n",
        "preprocessed_img2 = preprocess_signature(img2, canvas_size, input_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.132961Z",
          "iopub.execute_input": "2023-05-21T16:30:54.135389Z",
          "iopub.status.idle": "2023-05-21T16:30:54.332482Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.135346Z",
          "shell.execute_reply": "2023-05-21T16:30:54.331469Z"
        },
        "trusted": true,
        "id": "VyWyryNMJq_N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows pre-processed samples of the two users\n",
        "\n",
        "f, ax = plt.subplots(2,1, figsize=(15,10))\n",
        "ax[0].imshow(preprocessed_img1)\n",
        "ax[1].imshow(preprocessed_img2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.337605Z",
          "iopub.execute_input": "2023-05-21T16:30:54.340102Z",
          "iopub.status.idle": "2023-05-21T16:30:54.905824Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.340058Z",
          "shell.execute_reply": "2023-05-21T16:30:54.904833Z"
        },
        "trusted": true,
        "id": "HvEDVZaDJq_N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataaset"
      ],
      "metadata": {
        "id": "arFYRyP6Jq_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignatureDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, canvas_size, dim=(256, 256)):\n",
        "        self.df  = df\n",
        "        self.real_file_names = df[\"image_real_paths\"].values\n",
        "        self.forged_file_names = df[\"image_forged_paths\"].values\n",
        "        self.labels = df[\"label\"].values\n",
        "        self.dim = dim\n",
        "        self.canvas_size=canvas_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        # getting the image path\n",
        "        real_file_path = self.real_file_names[index]\n",
        "        forged_file_path = self.forged_file_names[index]\n",
        "\n",
        "        img1 = load_signature(real_file_path)\n",
        "        img2 = load_signature(forged_file_path)\n",
        "\n",
        "        img1 = preprocess_signature(img1, canvas_size, self.dim)\n",
        "        img2 = preprocess_signature(img2, canvas_size, self.dim)\n",
        "\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "\n",
        "        return torch.tensor(img1), torch.tensor(img2), label.float()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.91054Z",
          "iopub.execute_input": "2023-05-21T16:30:54.91293Z",
          "iopub.status.idle": "2023-05-21T16:30:54.927386Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.912889Z",
          "shell.execute_reply": "2023-05-21T16:30:54.926373Z"
        },
        "trusted": true,
        "id": "OJ36SMUyJq_N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SignatureDataset(train, canvas_size, input_size)\n",
        "img1, img2, _ = train_dataset[1]\n",
        "\n",
        "f, ax = plt.subplots(2,1, figsize=(15,10))\n",
        "ax[0].imshow(img1)\n",
        "ax[1].imshow(img2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:54.933153Z",
          "iopub.execute_input": "2023-05-21T16:30:54.935654Z",
          "iopub.status.idle": "2023-05-21T16:30:55.60158Z",
          "shell.execute_reply.started": "2023-05-21T16:30:54.93559Z",
          "shell.execute_reply": "2023-05-21T16:30:55.600561Z"
        },
        "trusted": true,
        "id": "0PolA3JCJq_O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive loss"
      ],
      "metadata": {
        "id": "gp7URBqDJq_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.cosine_similarity(F.normalize(output1), F.normalize(output2))\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "\n",
        "\n",
        "        return loss_contrastive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:55.606973Z",
          "iopub.execute_input": "2023-05-21T16:30:55.609364Z",
          "iopub.status.idle": "2023-05-21T16:30:55.621643Z",
          "shell.execute_reply.started": "2023-05-21T16:30:55.609324Z",
          "shell.execute_reply": "2023-05-21T16:30:55.620521Z"
        },
        "trusted": true,
        "id": "6y4VN6WKJq_O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "8sArXeY-Jq_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigNet(nn.Module):\n",
        "    \"\"\" SigNet model, from https://arxiv.org/abs/1705.05787\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SigNet, self).__init__()\n",
        "\n",
        "        self.feature_space_size = 2048\n",
        "\n",
        "        self.conv_layers = nn.Sequential(OrderedDict([\n",
        "            ('conv1', conv_bn_mish(1, 96, 11, stride=4)),\n",
        "            ('maxpool1', nn.MaxPool2d(3, 2)),\n",
        "            ('conv2', conv_bn_mish(96, 256, 5, pad=2)),\n",
        "            ('maxpool2', nn.MaxPool2d(3, 2)),\n",
        "            ('conv3', conv_bn_mish(256, 384, 3, pad=1)),\n",
        "            ('conv4', conv_bn_mish(384, 384, 3, pad=1)),\n",
        "            ('conv5', conv_bn_mish(384, 256, 3, pad=1)),\n",
        "            ('maxpool3', nn.MaxPool2d(3, 2)),\n",
        "        ]))\n",
        "\n",
        "        self.fc_layers = nn.Sequential(OrderedDict([\n",
        "            ('fc1', linear_bn_mish(256 * 3 * 5, 2048)),\n",
        "            ('fc2', linear_bn_mish(self.feature_space_size, self.feature_space_size)),\n",
        "        ]))\n",
        "\n",
        "    def forward_once(self, img):\n",
        "        x = self.conv_layers(img)\n",
        "        x = x.view(x.shape[0], 256 * 3 * 5)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "\n",
        "        # Inputs need to have 4 dimensions (batch x channels x height x width), and also be between [0, 1]\n",
        "        img1 = img1.view(-1, 1, 150, 220).float().div(255)\n",
        "        img2 = img2.view(-1, 1, 150, 220).float().div(255)\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(img1)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(img2)\n",
        "        return output1, output2\n",
        "\n",
        "\n",
        "def conv_bn_mish(in_channels, out_channels, kernel_size,  stride=1, pad=0):\n",
        "    return nn.Sequential(OrderedDict([\n",
        "        ('conv', nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False)),\n",
        "        ('bn', nn.BatchNorm2d(out_channels)),\n",
        "        ('mish', nn.Mish()),\n",
        "    ]))\n",
        "\n",
        "\n",
        "def linear_bn_mish(in_features, out_features):\n",
        "    return nn.Sequential(OrderedDict([\n",
        "        ('fc', nn.Linear(in_features, out_features, bias=False)),  # Bias is added after BN\n",
        "        ('bn', nn.BatchNorm1d(out_features)),\n",
        "        ('mish', nn.Mish()),\n",
        "    ]))\n",
        "\n",
        "class SiameseModel(nn.Module):\n",
        "    \"\"\" SigNet model, from https://arxiv.org/abs/1705.05787\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SiameseModel, self).__init__()\n",
        "\n",
        "        self.model = SigNet()\n",
        "        state_dict, _, _ = torch.load(\"../input/pretrained-signature-weights/signet.pth\")\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "        if CFG.projection2d:\n",
        "            self.probs = nn.Linear(4, 1)\n",
        "        else:\n",
        "            self.probs = nn.Linear(self.model.feature_space_size*2, 1)\n",
        "        self.projection2d = nn.Linear(self.model.feature_space_size, 2)\n",
        "\n",
        "    def forward_once(self, img):\n",
        "        x = self.model.forward_once(img)\n",
        "        return x\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "\n",
        "        # Inputs need to have 4 dimensions (batch x channels x height x width), and also be between [0, 1]\n",
        "        # forward pass of input 1\n",
        "        img1 = img1.view(-1, 1, 150, 220).float().div(255)\n",
        "        img2 = img2.view(-1, 1, 150, 220).float().div(255)\n",
        "        embedding1 = self.forward_once(img1)\n",
        "        # forward pass of input 2\n",
        "        embedding2 = self.forward_once(img2)\n",
        "\n",
        "        if CFG.projection2d:\n",
        "            #print(\"Project embeddings into 2d space\")\n",
        "            embedding1 = self.projection2d(embedding1)\n",
        "            embedding2 = self.projection2d(embedding2)\n",
        "            # Classification\n",
        "            output = torch.cat([embedding1, embedding2], dim=1)\n",
        "            output= self.probs(output)\n",
        "            return embedding1, embedding2, output\n",
        "        else:\n",
        "            # Classification\n",
        "            output = torch.cat([embedding1, embedding2], dim=1)\n",
        "            print(output.shape)\n",
        "            output= self.probs(output)\n",
        "            return embedding1, embedding2, output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:55.627947Z",
          "iopub.execute_input": "2023-05-21T16:30:55.631094Z",
          "iopub.status.idle": "2023-05-21T16:30:55.669265Z",
          "shell.execute_reply.started": "2023-05-21T16:30:55.631046Z",
          "shell.execute_reply": "2023-05-21T16:30:55.667966Z"
        },
        "trusted": true,
        "id": "MUbjMhO3Jq_O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "ZPAsJH6hJq_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train_fn(fold, train_loader, model, criterions, optimizer, epoch, scheduler, device):\n",
        "\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, (img1, img2, labels) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        img1 = img1.to(device).float()\n",
        "        img2 = img2.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        out1, out2, preds = model(img1, img2)\n",
        "        loss1 = criterions[0](out1, out2, labels)\n",
        "        loss2 = criterions[1](preds.squeeze(1), labels)\n",
        "        loss = (loss1 + loss2) / 2\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        # measure elapsed time\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print(f'Epoch: [{epoch}][{step}/{len(train_loader)}] ', end='')\n",
        "            print(f'Elapsed: {timeSince(start, float(step+1)/len(train_loader))} ', end='')\n",
        "            print(f'Loss: {losses.val:.4f}({losses.avg:.4f}) ', end='')\n",
        "            print(f'Grad: {grad_norm:.4f} ', end='')\n",
        "            if scheduler:\n",
        "                print(f'LR: {scheduler.get_last_lr()[0]:.6f}  ')\n",
        "            else:\n",
        "                print(f'LR: {CFG.lr}')\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:55.675443Z",
          "iopub.execute_input": "2023-05-21T16:30:55.678465Z",
          "iopub.status.idle": "2023-05-21T16:30:55.708763Z",
          "shell.execute_reply.started": "2023-05-21T16:30:55.678423Z",
          "shell.execute_reply": "2023-05-21T16:30:55.707451Z"
        },
        "trusted": true,
        "id": "sSnrhTqJJq_O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "lWGp57QTJq_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# loader\n",
        "# ====================================================\n",
        "seed_torch(seed=CFG.seed)\n",
        "\n",
        "\n",
        "train_dataset = SignatureDataset(train, CFG.canvas_size, dim=(256, 256))\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "\n",
        "# ====================================================\n",
        "# scheduler\n",
        "# ====================================================\n",
        "def get_scheduler(optimizer):\n",
        "    if CFG.scheduler=='ReduceLROnPlateau':\n",
        "        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
        "    elif CFG.scheduler=='CosineAnnealingLR':\n",
        "        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
        "    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
        "        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.reduce_params)\n",
        "    elif CFG.scheduler=='OneCycleLR':\n",
        "        scheduler = OneCycleLR(optimizer, **CFG.onecycle_params)\n",
        "    return scheduler\n",
        "\n",
        "# ====================================================\n",
        "# model & optimizer\n",
        "# ====================================================\n",
        "model = SiameseModel()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
        "try:\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "except:\n",
        "    print('Learning scheduler is not used')\n",
        "\n",
        "# ====================================================\n",
        "# loop\n",
        "# ====================================================\n",
        "contrastive = ContrastiveLoss() # Contrastive loss\n",
        "binary_cross = nn.BCEWithLogitsLoss() # Binary cross entropy\n",
        "best_score = 0.\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(CFG.epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # train\n",
        "    avg_loss = train_fn(train, train_loader, model, [contrastive, binary_cross], optimizer, epoch, scheduler, device)\n",
        "\n",
        "\n",
        "    if isinstance(scheduler, ReduceLROnPlateau):\n",
        "        scheduler.step(avg_val_loss)\n",
        "    elif isinstance(scheduler, CosineAnnealingLR):\n",
        "        scheduler.step()\n",
        "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
        "        scheduler.step()\n",
        "    elif isinstance(scheduler, OneCycleLR):\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
        "\n",
        "\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
        "        torch.save({'model': model.state_dict()},\n",
        "                        OUTPUT_DIR+f'{CFG.model_name}_best_loss.pt')\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:30:55.715298Z",
          "iopub.execute_input": "2023-05-21T16:30:55.718246Z",
          "iopub.status.idle": "2023-05-21T16:32:26.010944Z",
          "shell.execute_reply.started": "2023-05-21T16:30:55.718204Z",
          "shell.execute_reply": "2023-05-21T16:32:26.00969Z"
        },
        "trusted": true,
        "id": "PYrFTXq7Jq_O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "yMbSscRYJq_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_torch(seed=CFG.seed)\n",
        "model = SiameseModel()\n",
        "model.load_state_dict(torch.load('./convnet_best_loss.pt')['model'])\n",
        "\n",
        "\n",
        "test_dataset = SignatureDataset(test, CFG.canvas_size, dim=(256, 256))\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "\n",
        "counter = 0\n",
        "label_dict = {1.0:'Forged', 0.0:'Original'}\n",
        "#CFG.projection2d=True\n",
        "model.eval()\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    img1, img2, label = data\n",
        "    concatenated = torch.cat((img1, img2),0)\n",
        "    with torch.no_grad():\n",
        "        op1, op2, confidence = model(img1.to('cpu'), img2.to('cpu'))\n",
        "    confidence = confidence.sigmoid().detach().to('cpu')\n",
        "    if label == 0.0:\n",
        "        confidence = 1 - confidence\n",
        "    cos_sim = F.cosine_similarity(op1, op2)\n",
        "\n",
        "    imshow(torchvision.utils.make_grid(concatenated.unsqueeze(1)), f'similarity: {cos_sim.item():.2f} Confidence: {confidence.item():.2f} Label: {label_dict[label.item()]}')\n",
        "    plt.savefig('siamese.png')\n",
        "    counter+=1\n",
        "    if counter==40:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-21T16:32:26.014069Z",
          "iopub.execute_input": "2023-05-21T16:32:26.014826Z",
          "iopub.status.idle": "2023-05-21T16:32:36.728532Z",
          "shell.execute_reply.started": "2023-05-21T16:32:26.01477Z",
          "shell.execute_reply": "2023-05-21T16:32:36.727444Z"
        },
        "trusted": true,
        "id": "dMLhMJWeJq_O"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}